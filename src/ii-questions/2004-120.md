---
course: Information Theory
course_year: II
question_number: 120
tags:
- II
- '2004'
- Information Theory
title: 'B2.14 '
year: 2004
---


For integer-valued random variables $X$ and $Y$, define the relative entropy $h_{Y}(X)$ of $X$ relative to $Y$.

Prove that $h_{Y}(X) \geqslant 0$, with equality if and only if $\mathbb{P}(X=x)=\mathbb{P}(Y=x)$ for all $x$.

By considering $Y$, a geometric random variable with parameter chosen appropriately, show that if the mean $\mathbb{E} X=\mu<\infty$, then

$$h(X) \leqslant(\mu+1) \log (\mu+1)-\mu \log \mu,$$

with equality if $X$ is geometric.