---
course: Principles of Statistics
course_year: II
question_number: 121
tags:
- II
- '2013'
- Principles of Statistics
title: 'Paper 1, Section II, K '
year: 2013
---



When the real parameter $\Theta$ takes value $\theta$, variables $X_{1}, X_{2}, \ldots$ arise independently from a distribution $P_{\theta}$ having density function $p_{\theta}(x)$ with respect to an underlying measure $\mu$. Define the score variable $U_{n}(\theta)$ and the information function $I_{n}(\theta)$ for estimation of $\Theta$ based on $\boldsymbol{X}^{n}:=\left(X_{1}, \ldots, X_{n}\right)$, and relate $I_{n}(\theta)$ to $i(\theta):=I_{1}(\theta)$.

State and prove the Cram√©r-Rao inequality for the variance of an unbiased estimator of $\Theta$. Under what conditions does this inequality become an equality? What is the form of the estimator in this case? [You may assume $\mathbb{E}_{\theta}\left\{U_{n}(\theta)\right\}=0, \operatorname{var}_{\theta}\left\{U_{n}(\theta)\right\}=I_{n}(\theta)$, and any further required regularity conditions, without comment.]

Let $\widehat{\Theta}_{n}$ be the maximum likelihood estimator of $\Theta$ based on $\boldsymbol{X}^{n}$. What is the asymptotic distribution of $n^{\frac{1}{2}}\left(\widehat{\Theta}_{n}-\Theta\right)$ when $\Theta=\theta$ ?

Suppose that, for each $n, \widehat{\Theta}_{n}$ is unbiased for $\Theta$, and the variance of $n^{\frac{1}{2}}\left(\widehat{\Theta}_{n}-\Theta\right)$ is exactly equal to its asymptotic variance. By considering the estimator $\alpha \widehat{\Theta}_{k}+(1-\alpha) \widehat{\Theta}_{n}$, or otherwise, show that, for $k<n, \operatorname{cov}_{\theta}\left(\widehat{\Theta}_{k}, \widehat{\Theta}_{n}\right)=\operatorname{var}_{\theta}\left(\widehat{\Theta}_{n}\right)$.