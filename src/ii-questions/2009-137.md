---
course: Statistical Modelling
course_year: II
question_number: 137
tags:
- II
- '2009'
- Statistical Modelling
title: 'Paper 4, Section II, I '
year: 2009
---



Consider the linear model $Y=X \beta+\varepsilon$, where $\varepsilon \sim N_{n}\left(0, \sigma^{2} I\right)$ and $X$ is an $n \times p$ matrix of full rank $p<n$. Find the form of the maximum likelihood estimator $\hat{\beta}$ of $\beta$, and derive its distribution assuming that $\sigma^{2}$ is known.

Assuming the prior $\pi\left(\beta, \sigma^{2}\right) \propto \sigma^{-2}$ find the joint posterior of $\left(\beta, \sigma^{2}\right)$ up to a normalising constant. Derive the posterior conditional distribution $\pi\left(\beta \mid \sigma^{2}, X, Y\right)$.

Comment on the distribution of $\hat{\beta}$ found above and the posterior conditional $\pi\left(\beta \mid \sigma^{2}, X, Y\right)$. Comment further on the predictive distribution of $y^{*}$ at input $x^{*}$ under both the maximum likelihood and Bayesian approaches.