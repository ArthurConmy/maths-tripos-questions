---
course: Principles of Statistics
course_year: II
question_number: 104
tags:
- II
- '2005'
- Principles of Statistics
title: '2.II.27I '
year: 2005
---


(i) Suppose that $X$ is a multivariate normal vector with mean $\mu \in \mathbb{R}^{d}$ and covariance matrix $\sigma^{2} I$, where $\mu$ and $\sigma^{2}$ are both unknown, and $I$ denotes the $d \times d$ identity matrix. Suppose that $\Theta_{0} \subset \Theta_{1}$ are linear subspaces of $\mathbb{R}^{d}$ of dimensions $d_{0}$ and $d_{1}$, where $d_{0}<d_{1}<d$. Let $P_{i}$ denote orthogonal projection onto $\Theta_{i}(i=0,1)$. Carefully derive the joint distribution of $\left(\left|X-P_{1} X\right|^{2},\left|P_{1} X-P_{0} X\right|^{2}\right)$ under the hypothesis $H_{0}: \mu \in \Theta_{0}$. How could you use this to make a test of $H_{0}$ against $H_{1}: \mu \in \Theta_{1}$ ?

(ii) Suppose that $I$ students take $J$ exams, and that the mark $X_{i j}$ of student $i$ in exam $j$ is modelled as

$$X_{i j}=m+\alpha_{i}+\beta_{j}+\varepsilon_{i j}$$

where $\sum_{i} \alpha_{i}=0=\sum_{j} \beta_{j}$, the $\varepsilon_{i j}$ are independent $N\left(0, \sigma^{2}\right)$, and the parameters $m, \alpha, \beta$ and $\sigma$ are unknown. Construct a test of $H_{0}: \beta_{j}=0$ for all $j$ against $H_{1}: \sum_{j} \beta_{j}=0$.