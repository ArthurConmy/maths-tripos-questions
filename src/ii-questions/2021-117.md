---
course: Principles of Statistics
course_year: II
question_number: 117
tags:
- II
- '2021'
- Principles of Statistics
title: 'Paper 1, Section II, J '
year: 2021
---



Let $X_{1}, \ldots, X_{n}$ be random variables with joint probability density function in a statistical model $\left\{f_{\theta}: \theta \in \mathbb{R}\right\}$.

(a) Define the Fisher information $I_{n}(\theta)$. What do we mean when we say that the Fisher information tensorises?

(b) Derive the relationship between the Fisher information and the derivative of the score function in a regular model.

(c) Consider the model defined by $X_{1}=\theta+\varepsilon_{1}$ and

$$X_{i}=\theta(1-\sqrt{\gamma})+\sqrt{\gamma} X_{i-1}+\sqrt{1-\gamma} \varepsilon_{i} \quad \text { for } i=2, \ldots, n$$

where $\varepsilon_{1}, \ldots, \varepsilon_{n}$ are i.i.d. $N(0,1)$ random variables, and $\gamma \in[0,1)$ is a known constant. Compute the Fisher information $I_{n}(\theta)$. For which values of $\gamma$ does the Fisher information tensorise? State a lower bound on the variance of an unbiased estimator $\hat{\theta}$ in this model.