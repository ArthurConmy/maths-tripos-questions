---
course: Principles of Statistics
course_year: II
question_number: 120
tags:
- II
- '2011'
- Principles of Statistics
title: 'Paper 3, Section II, $27 \mathrm{~K}$ '
year: 2011
---



Random variables $X_{1}, X_{2}, \ldots$ are independent and identically distributed from the exponential distribution $\mathcal{E}(\theta)$, with density function

$$p_{X}(x \mid \theta)=\theta e^{-\theta x} \quad(x>0),$$

when the parameter $\Theta$ takes value $\theta>0$. The following experiment is performed. First $X_{1}$ is observed. Thereafter, if $X_{1}=x_{1}, \ldots, X_{i}=x_{i}$ have been observed $(i \geqslant 1)$, a coin having probability $\alpha\left(x_{i}\right)$ of landing heads is tossed, where $\alpha: \mathbb{R} \rightarrow(0,1)$ is a known function and the coin toss is independent of the $X$ 's and previous tosses. If it lands heads, no further observations are made; if tails, $X_{i+1}$ is observed.

Let $N$ be the total number of $X$ 's observed, and $\mathbf{X}:=\left(X_{1}, \ldots, X_{N}\right)$. Write down the likelihood function for $\Theta$ based on data $\mathbf{X}=\left(x_{1}, \ldots, x_{n}\right)$, and identify a minimal sufficient statistic. What does the likelihood principle have to say about inference from this experiment?

Now consider the experiment that only records $Y:=X_{N}$. Show that the density function of $Y$ has the form

$$p_{Y}(y \mid \theta)=\exp \{a(y)-k(\theta)-\theta y\}$$

Assuming the function $a(\cdot)$ is twice differentiable and that both $p_{Y}(y \mid \theta)$ and $\partial p_{Y}(y \mid \theta) / \partial y$ vanish at 0 and $\infty$, show that $a^{\prime}(Y)$ is an unbiased estimator of $\Theta$, and find its variance.

Stating clearly any general results you use, deduce that

$$-k^{\prime \prime}(\theta) \mathbb{E}_{\theta}\left\{a^{\prime \prime}(Y)\right\} \geqslant 1 .$$