---
course: Principles of Statistics
course_year: II
question_number: 103
tags:
- II
- '2007'
- Principles of Statistics
title: '1.II.27I '
year: 2007
---


Suppose that $X$ has density $f(\cdot \mid \theta)$ where $\theta \in \Theta$. What does it mean to say that statistic $T \equiv T(X)$ is sufficient for $\theta$ ?

Suppose that $\theta=(\psi, \lambda)$, where $\psi$ is the parameter of interest, and $\lambda$ is a nuisance parameter, and that the sufficient statistic $T$ has the form $T=(C, S)$. What does it mean to say that the statistic $S$ is ancillary? If it is, how (according to the conditionality principle) do we test hypotheses on $\psi ?$ Assuming that the set of possible values for $X$ is discrete, show that $S$ is ancillary if and only if the density (probability mass function) $f(x \mid \psi, \lambda)$ factorises as

$$f(x \mid \psi, \lambda)=\varphi_{0}(x) \varphi_{C}(C(x), S(x), \psi) \varphi_{S}(S(x), \lambda)$$

for some functions $\varphi_{0}, \varphi_{C}$, and $\varphi_{S}$ with the properties

$$\sum_{x \in C^{-1}(c) \cap S^{-1}(s)} \varphi_{0}(x)=1=\sum_{s} \varphi_{S}(s, \lambda)=\sum_{s} \sum_{c} \varphi_{C}(c, s, \psi)$$

for all $c, s, \psi$, and $\lambda$.

Suppose now that $X_{1}, \ldots, X_{n}$ are independent observations from a $\Gamma(a, b)$ distribution, with density

$$f(x \mid a, b)=(b x)^{a-1} e^{-b x} b I_{\{x>0\}} / \Gamma(a) .$$

Assuming that the criterion (*) holds also for observations which are not discrete, show that it is not possible to find $(C(X), S(X))$ sufficient for $(a, b)$ such that $S$ is ancillary when $b$ is regarded as a nuisance parameter, and $a$ is the parameter of interest.