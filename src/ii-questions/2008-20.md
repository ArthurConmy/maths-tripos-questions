---
course: Coding and Cryptography
course_year: II
question_number: 20
tags:
- II
- '2008'
- Coding and Cryptography
title: '1.II.12G '
year: 2008
---


State Shannon's Noisy Coding Theorem for a binary symmetric channel.

Define the mutual information of two discrete random variables $X$ and $Y$. Prove that the mutual information is symmetric and non-negative. Define also the information capacity of a channel.

A channel transmits numbers chosen from the alphabet $\mathcal{A}=\{0,1,2\}$ and has transition matrix

$$\left(\begin{array}{ccc}
1-2 \beta & \beta & \beta \\
\beta & 1-2 \beta & \beta \\
\beta & \beta & 1-2 \beta
\end{array}\right)$$

for a number $\beta$ with $0 \leqslant \beta \leqslant \frac{1}{3}$. Calculate the information capacity of the channel.