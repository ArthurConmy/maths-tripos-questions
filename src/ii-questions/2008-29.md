---
course: Statistical Modelling
course_year: II
question_number: 29
tags:
- II
- '2008'
- Statistical Modelling
title: '4.II.13J '
year: 2008
---


Consider the following generalized linear model for responses $y_{1}, \ldots, y_{n}$ as a function of explanatory variables $x_{1}, \ldots, x_{n}$, where $x_{i}=\left(x_{i 1}, \ldots, x_{i p}\right)^{\top}$ for $i=1, \ldots, n$. The responses are modelled as observed values of independent random variables $Y_{1}, \ldots, Y_{n}$, with

$$Y_{i} \sim \operatorname{ED}\left(\mu_{i}, \sigma_{i}^{2}\right), \quad g\left(\mu_{i}\right)=x_{i}^{\top} \beta, \quad \sigma_{i}^{2}=\sigma^{2} a_{i},$$

Here, $g$ is a given link function, $\beta$ and $\sigma^{2}$ are unknown parameters, and the $a_{i}$ are treated as known.

[Hint: recall that we write $Y \sim E D\left(\mu, \sigma^{2}\right)$ to mean that $Y$ has density function of the form

$$f\left(y ; \mu, \sigma^{2}\right)=a\left(\sigma^{2}, y\right) \exp \left\{\frac{1}{\sigma^{2}}[\theta(\mu) y-K(\theta(\mu))]\right\}$$

for given functions a and $\theta .]$

[ You may use without proof the facts that, for such a random variable $Y$,

$$\left.E(Y)=K^{\prime}(\theta(\mu)), \quad \operatorname{var}(Y)=\sigma^{2} K^{\prime \prime}(\theta(\mu)) \equiv \sigma^{2} V(\mu) .\right]$$

Show that the score vector and Fisher information matrix have entries:

$$U_{j}(\beta)=\sum_{i=1}^{n} \frac{\left(y_{i}-\mu_{i}\right) x_{i j}}{\sigma_{i}^{2} V\left(\mu_{i}\right) g^{\prime}\left(\mu_{i}\right)}, \quad j=1, \ldots, p$$

and

$$i_{j k}(\beta)=\sum_{i=1}^{n} \frac{x_{i j} x_{i k}}{\sigma_{i}^{2} V\left(\mu_{i}\right)\left(g^{\prime}\left(\mu_{i}\right)\right)^{2}}, \quad j, k=1, \ldots, p$$

How do these expressions simplify when the canonical link is used?

Explain briefly how these two expressions can be used to obtain the maximum likelihood estimate $\hat{\beta}$ for $\beta$.