---
course: Principles of Statistics
course_year: II
question_number: 119
tags:
- II
- '2009'
- Principles of Statistics
title: 'Paper 4, Section II, I '
year: 2009
---



Consider the double dichotomy, where the loss is 0 for a correct decision and 1 for an incorrect decision. Describe the form of a Bayes decision rule. Assuming the equivalence of normal and extensive form analyses, deduce the Neyman-Pearson lemma.

For a problem with random variable $X$ and real parameter $\theta$, define monotone likelihood ratio (MLR) and monotone test.

Suppose the problem has MLR in a real statistic $T=t(X)$. Let $\phi$ be a monotone test, with power function $\gamma(\cdot)$, and let $\phi^{\prime}$ be any other test, with power function $\gamma^{\prime}(\cdot)$. Show that if $\theta_{1}>\theta_{0}$ and $\gamma\left(\theta_{0}\right)>\gamma^{\prime}\left(\theta_{0}\right)$, then $\gamma\left(\theta_{1}\right)>\gamma^{\prime}\left(\theta_{1}\right)$. Deduce that there exists $\theta^{*} \in[-\infty, \infty]$ such that $\gamma(\theta) \leqslant \gamma^{\prime}(\theta)$ for $\theta<\theta^{*}$, and $\gamma(\theta) \geqslant \gamma^{\prime}(\theta)$ for $\theta>\theta^{*}$.

For an arbitrary prior distribution $\Pi$ with density $\pi(\cdot)$, and an arbitrary value $\theta^{*}$, show that the posterior odds

$$\frac{\Pi\left(\theta>\theta^{*} \mid X=x\right)}{\Pi\left(\theta \leqslant \theta^{*} \mid X=x\right)}$$

is a non-decreasing function of $t(x)$.