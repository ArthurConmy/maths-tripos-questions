---
course: Computational Statistics and Statistical Modelling
course_year: II
question_number: 14
tags:
- II
- '2004'
- Computational Statistics and Statistical Modelling
title: 'A1.13 '
year: 2004
---


(i) Assume that the $n$-dimensional vector $Y$ may be written as $Y=X \beta+\epsilon$, where $X$ is a given $n \times p$ matrix of $\operatorname{rank} p, \beta$ is an unknown vector, and

$$\epsilon \sim N_{n}\left(0, \sigma^{2} I\right)$$

Let $Q(\beta)=(Y-X \beta)^{T}(Y-X \beta)$. Find $\hat{\beta}$, the least-squares estimator of $\beta$, and state without proof the joint distribution of $\hat{\beta}$ and $Q(\hat{\beta})$.

(ii) Now suppose that we have observations $\left(Y_{i j}, 1 \leqslant i \leqslant I, 1 \leqslant j \leqslant J\right)$ and consider the model

$$\Omega: Y_{i j}=\mu+\alpha_{i}+\beta_{j}+\epsilon_{i j},$$

where $\left(\alpha_{i}\right),\left(\beta_{j}\right)$ are fixed parameters with $\Sigma \alpha_{i}=0, \Sigma \beta_{j}=0$, and $\left(\epsilon_{i j}\right)$ may be assumed independent normal variables, with $\epsilon_{i j} \sim N\left(0, \sigma^{2}\right)$, where $\sigma^{2}$ is unknown.

(a) Find $\left(\hat{\alpha}_{i}\right),\left(\hat{\beta}_{j}\right)$, the least-squares estimators of $\left(\alpha_{i}\right),\left(\beta_{j}\right)$.

(b) Find the least-squares estimators of $\left(\alpha_{i}\right)$ under the hypothesis $H_{0}: \beta_{j}=0$ for all $j$.

(c) Quoting any general theorems required, explain carefully how to test $H_{0}$, assuming $\Omega$ is true.

(d) What would be the effect of fitting the model $\Omega_{1}: Y_{i j}=\mu+\alpha_{i}+\beta_{j}+\gamma_{i j}+\epsilon_{i j}$, where now $\left(\alpha_{i}\right),\left(\beta_{j}\right),\left(\gamma_{i j}\right)$ are all fixed unknown parameters, and $\left(\epsilon_{i j}\right)$ has the distribution given above?