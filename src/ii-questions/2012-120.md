---
course: Principles of Statistics
course_year: II
question_number: 120
tags:
- II
- '2012'
- Principles of Statistics
title: 'Paper 3, Section II, $27 \mathrm{~K}$ '
year: 2012
---



The parameter vector is $\Theta \equiv\left(\Theta_{1}, \Theta_{2}, \Theta_{3}\right)$, with $\Theta_{i}>0, \Theta_{1}+\Theta_{2}+\Theta_{3}=1$. Given $\boldsymbol{\Theta}=\boldsymbol{\theta} \equiv\left(\theta_{1}, \theta_{2}, \theta_{3}\right)$, the integer random vector $\boldsymbol{X}=\left(X_{1}, X_{2}, X_{3}\right)$ has a trinomial distribution, with probability mass function

$$p(\boldsymbol{x} \mid \boldsymbol{\theta})=\frac{n !}{x_{1} ! x_{2} ! x_{3} !} \theta_{1}^{x_{1}} \theta_{2}^{x_{2}} \theta_{3}^{x_{3}}, \quad\left(x_{i} \geqslant 0, \sum_{i=1}^{3} x_{i}=n\right)$$

Compute the score vector for the parameter $\Theta^{*}:=\left(\Theta_{1}, \Theta_{2}\right)$, and, quoting any relevant general result, use this to determine $\mathbb{E}\left(X_{i}\right)(i=1,2,3)$.

Considering (1) as an exponential family with mean-value parameter $\Theta^{*}$, what is the corresponding natural parameter $\boldsymbol{\Phi} \equiv\left(\Phi_{1}, \Phi_{2}\right)$ ?

Compute the information matrix $I$ for $\Theta^{*}$, which has $(i, j)$-entry

$$I_{i j}=-\mathbb{E}\left(\frac{\partial^{2} l}{\partial \theta_{i} \partial \theta_{j}}\right) \quad(i, j=1,2)$$

where $l$ denotes the log-likelihood function, based on $\boldsymbol{X}$, expressed in terms of $\left(\theta_{1}, \theta_{2}\right)$.

Show that the variance of $\log \left(X_{1} / X_{3}\right)$ is asymptotic to $n^{-1}\left(\theta_{1}^{-1}+\theta_{3}^{-1}\right)$ as $n \rightarrow \infty$. [Hint. The information matrix $I_{\Phi}$ for $\boldsymbol{\Phi}$ is $I^{-1}$ and the dispersion matrix of the maximum likelihood estimator $\widehat{\boldsymbol{\Phi}}$ behaves, asymptotically (for $n \rightarrow \infty$ ) as $I_{\Phi}^{-1}$.]