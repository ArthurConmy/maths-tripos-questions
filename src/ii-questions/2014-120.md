---
course: Principles of Statistics
course_year: II
question_number: 120
tags:
- II
- '2014'
- Principles of Statistics
title: 'Paper 3, Section II, J '
year: 2014
---



State and prove Wilks' theorem about testing the simple hypothesis $H_{0}: \theta=\theta_{0}$, against the alternative $H_{1}: \theta \in \Theta \backslash\left\{\theta_{0}\right\}$, in a one-dimensional regular parametric model $\{f(\cdot, \theta): \theta \in \Theta\}, \Theta \subseteq \mathbb{R}$. [You may use without proof the results from lectures on the consistency and asymptotic distribution of maximum likelihood estimators, as well as on uniform laws of large numbers. Necessary regularity conditions can be assumed without statement.]

Find the maximum likelihood estimator $\hat{\theta}_{n}$ based on i.i.d. observations $X_{1}, \ldots, X_{n}$ in a $N(0, \theta)$-model, $\theta \in \Theta=(0, \infty)$. Deduce the limit distribution as $n \rightarrow \infty$ of the sequence of statistics

$$-n\left(\log \left(\overline{X^{2}}\right)-\left(\overline{X^{2}}-1\right)\right)$$

where $\overline{X^{2}}=(1 / n) \sum_{i=1}^{n} X_{i}^{2}$ and $X_{1}, \ldots, X_{n}$ are i.i.d. $N(0,1)$.