---
course: Statistical Modelling
course_year: II
question_number: 134
tags:
- II
- '2010'
- Statistical Modelling
title: 'Paper 2, Section I, J '
year: 2010
---



Suppose you have a parametric model consisting of probability mass functions $f(y ; \theta), \theta \in \Theta \subset \mathbb{R}$. Given a sample $Y_{1}, \ldots, Y_{n}$ from $f(y ; \theta)$, define the maximum likelihood estimator $\hat{\theta}_{n}$ for $\theta$ and, assuming standard regularity conditions hold, state the asymptotic distribution of $\sqrt{n}\left(\hat{\theta}_{n}-\theta\right)$.

Compute the Fisher information of a single observation in the case where $f(y ; \theta)$ is the probability mass function of a Poisson random variable with parameter $\theta$. If $Y_{1}, \ldots, Y_{n}$ are independent and identically distributed random variables having a Poisson distribution with parameter $\theta$, show that $\bar{Y}=\frac{1}{n} \sum_{i=1}^{n} Y_{i}$ and $S=\frac{1}{n-1} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}$ are unbiased estimators for $\theta$. Without calculating the variance of $S$, show that there is no reason to prefer $S$ over $\bar{Y}$.

[You may use the fact that the asymptotic variance of $\sqrt{n}\left(\hat{\theta}_{n}-\theta\right)$ is a lower bound for the variance of any unbiased estimator.]