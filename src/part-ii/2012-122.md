---
course: Principles of Statistics
course_year: II
question_number: 122
tags:
- II
- '2012'
- Principles of Statistics
title: 'Paper 1, Section II, $28 \mathrm{~K}$ '
year: 2012
---



Prove that, if $T$ is complete sufficient for $\Theta$, and $S$ is a function of $T$, then $S$ is the minimum variance unbiased estimator of $\mathbb{E}(S \mid \Theta)$.

When the parameter $\Theta$ takes a value $\theta>0$, observables $\left(X_{1}, \ldots, X_{n}\right)$ arise independently from the exponential distribution $\mathcal{E}(\theta)$, having probability density function

$$p(x \mid \theta)=\theta e^{-\theta x} \quad(x>0) .$$

Show that the family of distributions

$$\Theta \sim \operatorname{Gamma}(\alpha, \beta) \quad(\alpha>0, \beta>0),$$

with probability density function

$$\pi(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha-1} e^{-\beta \theta} \quad(\theta>0),$$

is a conjugate family for Bayesian inference about $\Theta$ (where $\Gamma(\alpha)$ is the Gamma function).

Show that the expectation of $\Lambda:=\log \Theta$, under prior distribution (1), is $\psi(\alpha)-\log \beta$, where $\psi(\alpha):=(\mathrm{d} / \mathrm{d} \alpha) \log \Gamma(\alpha)$. What is the prior variance of $\Lambda$ ? Deduce the posterior expectation and variance of $\Lambda$, given $\left(X_{1}, \ldots, X_{n}\right)$.

Let $\tilde{\Lambda}$ denote the limiting form of the posterior expectation of $\Lambda$ as $\alpha, \beta \downarrow 0$. Show that $\tilde{\Lambda}$ is the minimum variance unbiased estimator of $\Lambda$. What is its variance?