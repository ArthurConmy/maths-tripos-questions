---
course: Principles of Statistics
course_year: II
question_number: 69
tags:
- II
- '2002'
- Principles of Statistics
title: 'A4.13 B4.15 '
year: 2002
---


(a) Let $X_{1}, \ldots, X_{n}$ be independent, identically distributed random variables from a one-parameter distribution with density function

$$f(x ; \theta)=h(x) g(\theta) \exp \{\theta t(x)\}, x \in \mathbb{R} .$$

Explain in detail how you would test

$$H_{0}: \theta=\theta_{0} \text { against } H_{1}: \theta \neq \theta_{0} \text {. }$$

What is the general form of a conjugate prior density for $\theta$ in a Bayesian analysis of this distribution?

(b) Let $Y_{1}, Y_{2}$ be independent Poisson random variables, with means $(1-\psi) \lambda$ and $\psi \lambda$ respectively, with $\lambda$ known.

Explain why the Conditionality Principle leads to inference about $\psi$ being drawn from the conditional distribution of $Y_{2}$, given $Y_{1}+Y_{2}$. What is this conditional distribution?

(c) Suppose $Y_{1}, Y_{2}$ have distributions as in (b), but that $\lambda$ is now unknown.

Explain in detail how you would test $H_{0}: \psi=\psi_{0}$ against $H_{1}: \psi \neq \psi_{0}$, and describe the optimality properties of your test.

[Any general results you use should be stated clearly, but need not be proved.]