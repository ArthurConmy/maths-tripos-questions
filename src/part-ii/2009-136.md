---
course: Statistical Modelling
course_year: II
question_number: 136
tags:
- II
- '2009'
- Statistical Modelling
title: 'Paper 3, Section $I$, I '
year: 2009
---



Consider the linear model $Y=X \beta+\varepsilon$, where $\varepsilon \sim N_{n}\left(0, \sigma^{2} I\right)$ and $X$ is an $n \times p$ matrix of full rank $p<n$. Suppose that the parameter $\beta$ is partitioned into $k$ sets as follows: $\beta^{\top}=\left(\beta_{1}^{\top} \cdots \beta_{k}^{\top}\right)$. What does it mean for a pair of sets $\beta_{i}, \beta_{j}, i \neq j$, to be orthogonal? What does it mean for all $k$ sets to be mutually orthogonal?

In the model

$$Y_{i}=\beta_{0}+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\varepsilon_{i}$$

where $\varepsilon_{i} \sim N\left(0, \sigma^{2}\right)$ are independent and identically distributed, find necessary and sufficient conditions on $x_{11}, \ldots, x_{n 1}, x_{12}, \ldots, x_{n 2}$ for $\beta_{0}, \beta_{1}$ and $\beta_{2}$ to be mutually orthogonal.

If $\beta_{0}, \beta_{1}$ and $\beta_{2}$ are mutually orthogonal, what consequence does this have for the joint distribution of the corresponding maximum likelihood estimators $\hat{\beta}_{0}, \hat{\beta}_{1}$ and $\hat{\beta}_{2}$ ?