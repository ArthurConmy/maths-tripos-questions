---
course: Principles of Statistics
course_year: II
question_number: 103
tags:
- II
- '2006'
- Principles of Statistics
title: '1.II $. 27 \mathrm{~J} \quad$ '
year: 2006
---


(a) What is a loss function? What is a decision rule? What is the risk function of a decision rule? What is the Bayes risk of a decision rule with respect to a prior $\pi$ ?

(b) Let $\theta \mapsto R(\theta, d)$ denote the risk function of decision rule $d$, and let $r(\pi, d)$ denote the Bayes risk of decision rule $d$ with respect to prior $\pi$. Suppose that $d^{*}$ is a decision rule and $\pi_{0}$ is a prior over the parameter space $\Theta$ with the two properties

(i) $r\left(\pi_{0}, d^{*}\right)=\min _{d} r\left(\pi_{0}, d\right)$

(ii) $\sup _{\theta} R\left(\theta, d^{*}\right)=r\left(\pi_{0}, d^{*}\right)$.

Prove that $d^{*}$ is minimax.

(c) Suppose now that $\Theta=\mathcal{A}=\mathbb{R}$, where $\mathcal{A}$ is the space of possible actions, and that the loss function is

$$L(\theta, a)=\exp (-\lambda a \theta),$$

where $\lambda$ is a positive constant. If the law of the observation $X$ given parameter $\theta$ is $N\left(\theta, \sigma^{2}\right)$, where $\sigma>0$ is known, show (using (b) or otherwise) that the rule

$$d^{*}(x)=x / \sigma^{2} \lambda$$

is minimax.