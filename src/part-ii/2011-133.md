---
course: Statistical Modelling
course_year: II
question_number: 133
tags:
- II
- '2011'
- Statistical Modelling
title: 'Paper 1, Section I, J '
year: 2011
---



Let $Y_{1}, \ldots, Y_{n}$ be independent identically distributed random variables with model function $f(y, \theta), y \in \mathcal{Y}, \theta \in \Theta \subseteq \mathbb{R}$, and denote by $E_{\theta}$ and $\operatorname{Var}_{\theta}$ expectation and variance under $f(y, \theta)$, respectively. Define $U_{n}(\theta)=\sum_{i=1}^{n} \frac{\partial}{\partial \theta} \log f\left(Y_{i}, \theta\right)$. Prove that $E_{\theta} U_{n}(\theta)=0$. Show moreover that if $T=T\left(Y_{1}, \ldots, Y_{n}\right)$ is any unbiased estimator of $\theta$, then its variance satisfies $\operatorname{Var}_{\theta}(T) \geqslant\left(n \operatorname{Var}_{\theta}\left(U_{1}(\theta)\right)^{-1}\right.$. [You may use the Cauchy-Schwarz inequality without proof, and you may interchange differentiation and integration without justification if necessary.]