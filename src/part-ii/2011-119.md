---
course: Principles of Statistics
course_year: II
question_number: 119
tags:
- II
- '2011'
- Principles of Statistics
title: 'Paper 2, Section II, K '
year: 2011
---



Random variables $X_{1}, \ldots, X_{n}$ are independent and identically distributed from the normal distribution with unknown mean $\mathrm{M}$ and unknown precision (inverse variance) $H$. Show that the likelihood function, for data $X_{1}=x_{1}, \ldots, X_{n}=x_{n}$, is

$$L_{n}(\mu, h) \propto h^{n / 2} \exp \left(-\frac{1}{2} h\left\{n(\bar{x}-\mu)^{2}+S\right\}\right)$$

where $\bar{x}:=n^{-1} \sum_{i} x_{i}$ and $S:=\sum_{i}\left(x_{i}-\bar{x}\right)^{2}$.

A bivariate prior distribution for $(\mathrm{M}, H)$ is specified, in terms of hyperparameters $\left(\alpha_{0}, \beta_{0}, m_{0}, \lambda_{0}\right)$, as follows. The marginal distribution of $H$ is $\Gamma\left(\alpha_{0}, \beta_{0}\right)$, with density

$$\pi(h) \propto h^{\alpha_{0}-1} e^{-\beta_{0} h} \quad(h>0),$$

and the conditional distribution of $\mathrm{M}$, given $H=h$, is normal with mean $m_{0}$ and precision $\lambda_{0} h$.

Show that the conditional prior distribution of $H$, given $M=\mu$, is

$$H \mid \mathrm{M}=\mu \quad \sim\left(\alpha_{0}+\frac{1}{2}, \beta_{0}+\frac{1}{2} \lambda_{0}\left(\mu-m_{0}\right)^{2}\right)$$

Show that the posterior joint distribution of $(\mathrm{M}, H)$, given $X_{1}=x_{1}, \ldots, X_{n}=x_{n}$, has the same form as the prior, with updated hyperparameters $\left(\alpha_{n}, \beta_{n}, m_{n}, \lambda_{n}\right)$ which you should express in terms of the prior hyperparameters and the data.

[You may use the identity

$$p(t-a)^{2}+q(t-b)^{2}=(t-\delta)^{2}+p q(a-b)^{2}$$

where $p+q=1$ and $\delta=p a+q b$.]

Explain how you could implement Gibbs sampling to generate a random sample from the posterior joint distribution.