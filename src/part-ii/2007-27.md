---
course: Statistical Modelling
course_year: II
question_number: 27
tags:
- II
- '2007'
- Statistical Modelling
title: '3.I.5I '
year: 2007
---


Consider two possible experiments giving rise to observed data $y_{i j}$ where $i=1, \ldots, I, j=1, \ldots, J .$

1. The data are realizations of independent Poisson random variables, i.e.,

$$Y_{i j} \sim f_{1}\left(y_{i j} ; \mu_{i j}\right)=\frac{\mu_{i j}^{y_{i j}}}{y_{i j} !} \exp \left\{-\mu_{i j}\right\}$$

where $\mu_{i j}=\mu_{i j}(\beta)$, with $\beta$ an unknown (possibly vector) parameter. Write $\hat{\beta}$ for the maximum likelihood estimator (m.l.e.) of $\beta$ and $\hat{y}_{i j}=\mu_{i j}(\hat{\beta})$ for the $(i, j)$ th fitted value under this model.

2. The data are components of a realization of a multinomial random 'vector'

$$Y \sim f_{2}\left(\left(y_{i j}\right) ; n,\left(p_{i j}\right)\right)=n ! \prod_{i=1}^{I} \prod_{j=1}^{J} \frac{p_{i j}^{y_{i j}}}{y_{i j} !}$$

where the $y_{i j}$ are non-negative integers with

$$\sum_{i=1}^{I} \sum_{j=1}^{J} y_{i j}=n \quad \text { and } \quad p_{i j}(\beta)=\frac{\mu_{i j}(\beta)}{n}$$

Write $\beta^{*}$ for the m.l.e. of $\beta$ and $y_{i j}^{*}=n p_{i j}\left(\beta^{*}\right)$ for the $(i, j)$ th fitted value under this model.

Show that, if

$$\sum_{i=1}^{I} \sum_{j=1}^{J} \hat{y}_{i j}=n$$

then $\hat{\beta}=\beta^{*}$ and $\hat{y}_{i j}=y_{i j}^{*}$ for all $i, j$. Explain the relevance of this result in the context of fitting multinomial models within a generalized linear model framework.