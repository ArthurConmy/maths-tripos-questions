---
course: Principles of Statistics
course_year: II
question_number: 65
tags:
- II
- '2004'
- Principles of Statistics
title: 'A3.12 B3.15 '
year: 2004
---


(i) What is a sufficient statistic? What is a minimal sufficient statistic? Explain the terms nuisance parameter and ancillary statistic.

(ii) Let $U_{1}, \ldots, U_{n}$ be independent random variables with common uniform( $([0,1])$ distribution, and suppose you observe $X_{i} \equiv a U_{i}^{-\beta}, i=1, \ldots, n$, where the positive parameters $a, \beta$ are unknown. Write down the joint density of $X_{1}, \ldots, X_{n}$ and prove that the statistic

$$(m, p) \equiv\left(\min _{1 \leqslant j \leqslant n}\left\{X_{j}\right\}, \prod_{j=1}^{n} X_{j}\right)$$

is minimal sufficient for $(a, \beta)$. Find the maximum-likelihood estimator $(\hat{a}, \hat{\beta})$ of $(a, \beta)$.

Regarding $\beta$ as the parameter of interest and $a$ as the nuisance parameter, is $m$ ancillary? Find the mean and variance of $\hat{\beta}$. Hence find an unbiased estimator of $\beta$.