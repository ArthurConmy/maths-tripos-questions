---
course: Principles of Statistics
course_year: II
question_number: 112
tags:
- II
- '2015'
- Principles of Statistics
title: 'Paper 3, Section II, J '
year: 2015
---



Define what it means for an estimator $\hat{\theta}$ of an unknown parameter $\theta$ to be consistent.

Let $S_{n}$ be a sequence of random real-valued continuous functions defined on $\mathbb{R}$ such that, as $n \rightarrow \infty, S_{n}(\theta)$ converges to $S(\theta)$ in probability for every $\theta \in \mathbb{R}$, where $S: \mathbb{R} \rightarrow \mathbb{R}$ is non-random. Suppose that for some $\theta_{0} \in \mathbb{R}$ and every $\varepsilon>0$ we have

$$S\left(\theta_{0}-\varepsilon\right)<0<S\left(\theta_{0}+\varepsilon\right),$$

and that $S_{n}$ has exactly one zero $\hat{\theta}_{n}$ for every $n \in \mathbb{N}$. Show that $\hat{\theta}_{n} \rightarrow \theta_{0}$ as $n \rightarrow \infty$, and deduce from this that the maximum likelihood estimator (MLE) based on observations $X_{1}, \ldots, X_{n}$ from a $N(\theta, 1), \theta \in \mathbb{R}$ model is consistent.

Now consider independent observations $\mathbf{X}_{1}, \ldots, \mathbf{X}_{n}$ of bivariate normal random vectors

$$\mathbf{X}_{i}=\left(X_{1 i}, X_{2 i}\right)^{T} \sim N_{2}\left[\left(\mu_{i}, \mu_{i}\right)^{T}, \sigma^{2} I_{2}\right], \quad i=1, \ldots, n,$$

where $\mu_{i} \in \mathbb{R}, \sigma>0$ and $I_{2}$ is the $2 \times 2$ identity matrix. Find the MLE $\hat{\mu}=\left(\hat{\mu}_{1}, \ldots, \hat{\mu}_{n}\right)^{T}$ of $\mu=\left(\mu_{1}, \ldots, \mu_{n}\right)^{T}$ and show that the MLE of $\sigma^{2}$ equals

$$\hat{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n} s_{i}^{2}, \quad s_{i}^{2}=\frac{1}{2}\left[\left(X_{1 i}-\hat{\mu}_{i}\right)^{2}+\left(X_{2 i}-\hat{\mu}_{i}\right)^{2}\right]$$

Show that $\hat{\sigma}^{2}$ is not consistent for estimating $\sigma^{2}$. Explain briefly why the MLE fails in this model.

[You may use the Law of Large Numbers without proof.]