---
course: Principles of Statistics
course_year: II
question_number: 68
tags:
- II
- '2002'
- Principles of Statistics
title: 'A3.12 B3.15 '
year: 2002
---


(i) Describe in detail how to perform the Wald, score and likelihood ratio tests of a simple null hypothesis $H_{0}: \theta=\theta_{0}$ given a random sample $X_{1}, \ldots, X_{n}$ from a regular oneparameter density $f(x ; \theta)$. In each case you should specify the asymptotic null distribution of the test statistic.

(ii) Let $X_{1}, \ldots, X_{n}$ be an independent, identically distributed sample from a distribution $F$, and let $\hat{\theta}\left(X_{1}, \ldots, X_{n}\right)$ be an estimator of a parameter $\theta$ of $F$.

Explain what is meant by: (a) the empirical distribution function of the sample; (b) the bootstrap estimator of the bias of $\hat{\theta}$, based on the empirical distribution function. Explain how a bootstrap estimator of the distribution function of $\hat{\theta}-\theta$ may be used to construct an approximate $1-\alpha$ confidence interval for $\theta$.

Suppose the parameter of interest is $\theta=\mu^{2}$, where $\mu$ is the mean of $F$, and the estimator is $\hat{\theta}=\bar{X}^{2}$, where $\bar{X}=n^{-1} \sum_{i=1}^{n} X_{i}$ is the sample mean.

Derive an explicit expression for the bootstrap estimator of the bias of $\hat{\theta}$ and show that it is biased as an estimator of the true bias of $\hat{\theta}$.

Let $\hat{\theta}_{i}$ be the value of the estimator $\hat{\theta}\left(X_{1}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right)$ computed from the sample of size $n-1$ obtained by deleting $X_{i}$ and let $\hat{\theta} . n^{-1} \sum_{i=1}^{n} \hat{\theta}_{i}$. The jackknife estimator of the bias of $\hat{\theta}$ is

$$b_{J}=(n-1)(\hat{\theta} .-\hat{\theta}) .$$

Derive the jackknife estimator $b_{J}$ for the case $\hat{\theta}=\bar{X}^{2}$, and show that, as an estimator of the true bias of $\hat{\theta}$, it is unbiased.