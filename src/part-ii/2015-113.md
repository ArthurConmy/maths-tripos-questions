---
course: Principles of Statistics
course_year: II
question_number: 113
tags:
- II
- '2015'
- Principles of Statistics
title: 'Paper 2, Section II, J '
year: 2015
---



Consider a random variable $X$ arising from the binomial distribution $\operatorname{Bin}(n, \theta)$, $\theta \in \Theta=[0,1]$. Find the maximum likelihood estimator $\hat{\theta}_{M L E}$ and the Fisher information $I(\theta)$ for $\theta \in \Theta$.

Now consider the following priors on $\Theta$ :

(i) a uniform $U([0,1])$ prior on $[0,1]$,

(ii) a prior with density $\pi(\theta)$ proportional to $\sqrt{I(\theta)}$,

(iii) a $\operatorname{Beta}(\sqrt{n} / 2, \sqrt{n} / 2)$ prior.

Find the means $E[\theta \mid X]$ and modes $m_{\theta} \mid X$ of the posterior distributions corresponding to the prior distributions (i)-(iii). Which of these posterior decision rules coincide with $\hat{\theta}_{M L E}$ ? Which one is minimax for quadratic risk? Justify your answers.

[You may use the following properties of the $\operatorname{Beta}(a, b)(a>0, b>0)$ distribution. Its density $f(x ; a, b), x \in[0,1]$, is proportional to $x^{a-1}(1-x)^{b-1}$, its mean is equal to $a /(a+b)$, and its mode is equal to

$$\frac{\max (a-1,0)}{\max (a, 1)+\max (b, 1)-2}$$

provided either $a>1$ or $b>1$.

You may further use the fact that a unique Bayes rule of constant risk is a unique minimax rule for that risk.]