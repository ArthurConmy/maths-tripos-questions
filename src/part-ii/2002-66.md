---
course: Principles of Statistics
course_year: II
question_number: 66
tags:
- II
- '2002'
- Principles of Statistics
title: 'A1.12 B1.15 '
year: 2002
---


(i) Explain in detail the minimax and Bayes principles of decision theory.

Show that if $d(X)$ is a Bayes decision rule for a prior density $\pi(\theta)$ and has constant risk function, then $d(X)$ is minimax.

(ii) Let $X_{1}, \ldots, X_{p}$ be independent random variables, with $X_{i} \sim N\left(\mu_{i}, 1\right), i=1, \ldots, p$.

Consider estimating $\mu=\left(\mu_{1}, \ldots, \mu_{p}\right)^{T}$ by $d=\left(d_{1}, \ldots, d_{p}\right)^{T}$, with loss function

$$L(\mu, d)=\sum_{i=1}^{p}\left(\mu_{i}-d_{i}\right)^{2}$$

What is the risk function of $X=\left(X_{1}, \ldots, X_{p}\right)^{T} ?$

Consider the class of estimators of $\mu$ of the form

$$d^{a}(X)=\left(1-\frac{a}{X^{T} X}\right) X$$

indexed by $a \geqslant 0$. Find the risk function of $d^{a}(X)$ in terms of $E\left(1 / X^{T} X\right)$, which you should not attempt to evaluate, and deduce that $X$ is inadmissible. What is optimal value of $a$ ?

[You may assume Stein's Lemma, that for suitably behaved real-valued functions $h$,

$$\left.E\left\{\left(X_{i}-\mu_{i}\right) h(X)\right\}=E\left\{\frac{\partial h(X)}{\partial X_{i}}\right\} . \quad\right]$$