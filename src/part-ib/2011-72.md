---
course: Statistics
course_year: IB
question_number: 72
tags:
- IB
- '2011'
- Statistics
title: 'Paper 1, Section I, $\mathbf{7 H} \quad$ '
year: 2011
---



Consider the experiment of tossing a coin $n$ times. Assume that the tosses are independent and the coin is biased, with unknown probability $p$ of heads and $1-p$ of tails. A total of $X$ heads is observed.

(i) What is the maximum likelihood estimator $\widehat{p}$ of $p$ ?

Now suppose that a Bayesian statistician has the $\operatorname{Beta}(M, N)$ prior distribution for $p$.

(ii) What is the posterior distribution for $p$ ?

(iii) Assuming the loss function is $L(p, a)=(p-a)^{2}$, show that the statistician's point estimate for $p$ is given by

$$\frac{M+X}{M+N+n}$$

[The $\operatorname{Beta}(M, N)$ distribution has density $\frac{\Gamma(M+N)}{\Gamma(M) \Gamma(N)} x^{M-1}(1-x)^{N-1}$ for $0<x<1$ and $\left.\operatorname{mean} \frac{M}{M+N} .\right]$