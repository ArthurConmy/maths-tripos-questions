---
course: Statistics
course_year: IB
question_number: 75
tags:
- IB
- '2015'
- Statistics
title: 'Paper 1, Section II, H '
year: 2015
---



Suppose $X_{1}, \ldots, X_{n}$ are independent identically distributed random variables each with probability mass function $\mathbb{P}\left(X_{i}=x_{i}\right)=p\left(x_{i} ; \theta\right)$, where $\theta$ is an unknown parameter. State what is meant by a sufficient statistic for $\theta$. State the factorisation criterion for a sufficient statistic. State and prove the Rao-Blackwell theorem.

Suppose that $X_{1}, \ldots, X_{n}$ are independent identically distributed random variables with

$$\mathbb{P}\left(X_{i}=x_{i}\right)=\left(\begin{array}{c}
m \\
x_{i}
\end{array}\right) \theta^{x_{i}}(1-\theta)^{m-x_{i}}, \quad x_{i}=0, \ldots, m$$

where $m$ is a known positive integer and $\theta$ is unknown. Show that $\tilde{\theta}=X_{1} / m$ is unbiased for $\theta$.

Show that $T=\sum_{i=1}^{n} X_{i}$ is sufficient for $\theta$ and use the Rao-Blackwell theorem to find another unbiased estimator $\hat{\theta}$ for $\theta$, giving details of your derivation. Calculate the variance of $\hat{\theta}$ and compare it to the variance of $\tilde{\theta}$.

A statistician cannot remember the exact statement of the Rao-Blackwell theorem and calculates $\mathbb{E}\left(T \mid X_{1}\right)$ in an attempt to find an estimator of $\theta$. Comment on the suitability or otherwise of this approach, giving your reasons.

[Hint: If $a$ and $b$ are positive integers then, for $r=0,1, \ldots, a+b,\left(\begin{array}{c}a+b \\ r\end{array}\right)=$ $\left.\sum_{j=0}^{r}\left(\begin{array}{c}a \\ j\end{array}\right)\left(\begin{array}{c}b \\ r-j\end{array}\right) .\right]$