---
course: Statistics
course_year: IB
question_number: 70
tags:
- IB
- '2014'
- Statistics
title: 'Paper 1, Section I, $\mathbf{7 H} \quad$ '
year: 2014
---



Consider an estimator $\hat{\theta}$ of an unknown parameter $\theta$, and assume that $\mathbb{E}_{\theta}\left(\hat{\theta}^{2}\right)<\infty$ for all $\theta$. Define the bias and mean squared error of $\hat{\theta}$.

Show that the mean squared error of $\hat{\theta}$ is the sum of its variance and the square of its bias.

Suppose that $X_{1}, \ldots, X_{n}$ are independent identically distributed random variables with mean $\theta$ and variance $\theta^{2}$, and consider estimators of $\theta$ of the form $k \bar{X}$ where $\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$.

(i) Find the value of $k$ that gives an unbiased estimator, and show that the mean squared error of this unbiased estimator is $\theta^{2} / n$.

(ii) Find the range of values of $k$ for which the mean squared error of $k \bar{X}$ is smaller $\operatorname{than} \theta^{2} / n$.